# LangChain å…¥é—¨ç¤ºä¾‹ï¼šæ„å»ºä¸€ä¸ªç®€å•çš„ç¿»è¯‘åº”ç”¨

# 1. å¯¼å…¥å¿…è¦çš„æ¨¡å—
import os
import getpass
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate

# 2. è®¾ç½®APIå¯†é’¥ï¼ˆä»¥MoonShot Geminiä¸ºä¾‹ï¼‰
# å¦‚æœä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œè¯·ç›¸åº”è°ƒæ•´APIå¯†é’¥è®¾ç½®
if not os.environ.get("MOONSHOT_API_KEY"):
    os.environ["MOONSHOT_API_KEY"] = getpass.getpass("è¯·è¾“å…¥æ‚¨çš„MOONSHOT APIå¯†é’¥: ")

# 3. åˆå§‹åŒ–èŠå¤©æ¨¡å‹
def setup_model():
    """è®¾ç½®å¹¶è¿”å›èŠå¤©æ¨¡å‹"""
    try:
        model = init_chat_model(
            "qwen-plus-2025-07-28", 
            model_provider="openai",
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1",
            api_key = "sk-282660fdc8cc4460943f2da2a86d3d01"
        )
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
        return model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

# 4. åŸºç¡€ä½¿ç”¨ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹
def basic_chat_example(model):
    """åŸºç¡€èŠå¤©ç¤ºä¾‹"""
    print("\n=== åŸºç¡€èŠå¤©ç¤ºä¾‹ ===")
    
    # åˆ›å»ºæ¶ˆæ¯
    messages = [
        SystemMessage("ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"),
        HumanMessage("ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")
    ]
    
    # è°ƒç”¨æ¨¡å‹
    response = model.invoke(messages)
    print(f"AIå›å¤: {response.content}")

# 5. ä½¿ç”¨æç¤ºæ¨¡æ¿æ„å»ºç¿»è¯‘åº”ç”¨
def translation_app_example(model):
    """ç¿»è¯‘åº”ç”¨ç¤ºä¾‹"""
    print("\n=== ç¿»è¯‘åº”ç”¨ç¤ºä¾‹ ===")
    
    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{source_language}ç¿»è¯‘æˆ{target_language}ã€‚"),
        ("human", "{text}")
    ])
    
    # åˆ›å»ºç¿»è¯‘é“¾
    translation_chain = prompt_template | model
    
    # æµ‹è¯•ç¿»è¯‘
    result = translation_chain.invoke({
        "source_language": "ä¸­æ–‡",
        "target_language": "è‹±æ–‡", 
        "text": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­æ•£æ­¥å§ã€‚"
    })
    
    print(f"ç¿»è¯‘ç»“æœ: {result.content}")

# 6. æµå¼è¾“å‡ºç¤ºä¾‹
def streaming_example(model):
    """æµå¼è¾“å‡ºç¤ºä¾‹"""
    print("\n=== æµå¼è¾“å‡ºç¤ºä¾‹ ===")
    
    messages = [
        SystemMessage("ä½ æ˜¯ä¸€ä¸ªåˆ›æ„å†™ä½œåŠ©æ‰‹ã€‚"),
        HumanMessage("è¯·å†™ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„çŸ­æ•…äº‹ï¼Œå¤§çº¦100å­—ã€‚")
    ]
    
    print("AIæ­£åœ¨ç”Ÿæˆæ•…äº‹...")
    for chunk in model.stream(messages):
        print(chunk.content, end="", flush=True)
    print("\n")

# 7. æ‰¹é‡å¤„ç†ç¤ºä¾‹
def batch_processing_example(model):
    """æ‰¹é‡å¤„ç†ç¤ºä¾‹"""
    print("\n=== æ‰¹é‡å¤„ç†ç¤ºä¾‹ ===")
    
    # å‡†å¤‡å¤šä¸ªç¿»è¯‘ä»»åŠ¡
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š"),
        ("human", "{text}")
    ])
    
    chain = prompt_template | model
    
    # æ‰¹é‡ç¿»è¯‘
    texts = [
        {"text": "æ—©ä¸Šå¥½ï¼"},
        {"text": "è°¢è°¢ä½ çš„å¸®åŠ©ã€‚"},
        {"text": "ä»Šå¤©æ˜¯æ˜ŸæœŸä¸€ã€‚"}
    ]
    
    results = chain.batch(texts)
    
    for i, result in enumerate(results):
        print(f"åŸæ–‡: {texts[i]['text']} -> è¯‘æ–‡: {result.content}")

# 8. é”™è¯¯å¤„ç†ç¤ºä¾‹
def error_handling_example():
    """é”™è¯¯å¤„ç†ç¤ºä¾‹"""
    print("\n=== é”™è¯¯å¤„ç†ç¤ºä¾‹ ===")
    
    try:
        # å°è¯•ä½¿ç”¨é”™è¯¯çš„APIå¯†é’¥
        model = init_chat_model("gpt-4", model_provider="openai")
        response = model.invoke([HumanMessage("Hello")])
        print(f"å“åº”: {response.content}")
    except Exception as e:
        print(f"æ•è·åˆ°é”™è¯¯: {e}")
        print("æç¤ºï¼šè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®")

# 9. ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ æ¬¢è¿ä½¿ç”¨LangChainå…¥é—¨æ•™ç¨‹ï¼")
    print("=" * 50)
    
    # è®¾ç½®æ¨¡å‹
    model = setup_model()
    if not model:
        print("âŒ æ— æ³•åˆå§‹åŒ–æ¨¡å‹ï¼Œç¨‹åºé€€å‡º")
        return
    
    # è¿è¡Œå„ç§ç¤ºä¾‹
    basic_chat_example(model)
    translation_app_example(model)
    streaming_example(model)
    batch_processing_example(model)
    error_handling_example()
    
    print("\nğŸ‰ æ•™ç¨‹å®Œæˆï¼")
    print("æ¥ä¸‹æ¥æ‚¨å¯ä»¥ï¼š")
    print("1. å°è¯•ä¸åŒçš„æ¨¡å‹æä¾›å•†ï¼ˆOpenAIã€Anthropicç­‰ï¼‰")
    print("2. å­¦ä¹ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰")
    print("3. æ¢ç´¢LangGraphæ„å»ºå¤æ‚çš„ä»£ç†")
    print("4. ä½¿ç”¨LangSmithè¿›è¡Œåº”ç”¨ç›‘æ§")

# 10. å®ç”¨å·¥å…·å‡½æ•°
def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…å®Œæ•´"""
    print("\n=== ä¾èµ–æ£€æŸ¥ ===")
    
    required_packages = [
        'langchain',
        'langchain_core',
        'langchain_community'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package} å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {package} æœªå®‰è£…")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nè¯·å®‰è£…ç¼ºå¤±çš„åŒ…ï¼š")
        for package in missing_packages:
            print(f"pip install {package}")
    else:
        print("\nğŸ‰ æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å®‰è£…ï¼")

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    check_dependencies()
    
    # è¿è¡Œä¸»ç¨‹åº
    main()